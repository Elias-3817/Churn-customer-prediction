{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ff24848",
   "metadata": {},
   "source": [
    "## ðŸ“Š DATA UNDERSTANDING & CLEANING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c74885",
   "metadata": {},
   "source": [
    "\n",
    "In this notebook, we load the raw churn dataset, perform initial exploration to understand its structure, identify missing or inconsistent values, and clean/transform the data so that itâ€™s ready for feature engineering (Notebook 04)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aaf3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the packages we need\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a709ab",
   "metadata": {},
   "source": [
    "###  Load Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b6f559",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/customer_churn.csv\")\n",
    "#show first few rows to show if it loaded properly\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baccc4a1",
   "metadata": {},
   "source": [
    "### Initial Structure: Shape, Info, and Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740a224b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We check the number of rows and columns, data types, and non-null counts to see if any obvious issues pop up right away.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6c9e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identifying how many rows and columns are in the dataset\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "\n",
    "#Identifying the data types and non-null counts\n",
    "df.info()\n",
    "\n",
    "#Looking at the column names and trying to get more information generally about what we're working with\n",
    "print(\"\\nColumns:\\n\", df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500132c3",
   "metadata": {},
   "source": [
    "\n",
    "- The dataset contains **7,043 rows** and **21 columns**.\n",
    "- **No columns are missing values** (all non-null counts equal 7,043).\n",
    "- There are **18 object-type columns** and only **3 numeric columns** (`SeniorCitizen`, `tenure`, and `MonthlyCharges`).\n",
    "- Notably, **`TotalCharges` is read as an object** even though it should be numeric. We will need to convert it to a numeric type in a later step.\n",
    "- Most object columns correspond to categorical variables (e.g., `gender`, `Partner`, `Contract`, `Churn`), so weâ€™ll handle those appropriately during preprocessing.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800c86df",
   "metadata": {},
   "source": [
    "### Descriptive Statistics & Unique Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eb6df6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- Use `.describe()` on numeric columns to spot odd min/max.\n",
    "- Use `.nunique()` on every column to see how many distinct values each has (especially important for categorical features).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41a7b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric summary\n",
    "print(\"Numeric summary:\\n\")\n",
    "print(df.describe().T)\n",
    "\n",
    "# Unique counts per column\n",
    "print(\"\\nUnique counts per column:\")\n",
    "print(df.nunique().sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0655fe81",
   "metadata": {},
   "source": [
    "- From the `.describe()` summary, we observe that `MonthlyCharges` and `tenure` have a wide range of values, suggesting they may play an important role in predicting churn.\n",
    "- `SeniorCitizen` is binary but encoded as integers (0 or 1), so itâ€™s fine as-is for modeling.\n",
    "- The `TotalCharges` column has 6531 unique values, yet the dataset has 7043 rows â€” this discrepancy suggests **missing or non-numeric values** that need to be cleaned.\n",
    "- Categorical features mostly have 2 or 3 unique values, so theyâ€™ll be easy to encode later.\n",
    "- `customerID` is a unique identifier with no modeling value â€” weâ€™ll drop it eventually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254854f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking problematic values in TotalCharges\n",
    "print(\"Non-numeric 'TotalCharges' rows:\")\n",
    "print(df[pd.to_numeric(df[\"TotalCharges\"], errors=\"coerce\").isna()][[\"customerID\", \"tenure\", \"TotalCharges\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a84a66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert TotalCharges to numeric, forcing invalid strings to NaN\n",
    "df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"], errors='coerce')\n",
    "\n",
    "# Drop rows where TotalCharges is NaN\n",
    "df = df[df[\"TotalCharges\"].notnull()]\n",
    "\n",
    "# Reset index after dropping rows\n",
    "df.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2538866c",
   "metadata": {},
   "source": [
    "- We discovered that 11 rows had blank strings in the `TotalCharges` column, all with `tenure = 0`. These are likely new customers who haven't been billed yet.\n",
    "- We converted the column to numeric, coercing invalid entries to `NaN`, and then removed those rows since they don't offer meaningful information for churn prediction.\n",
    "- Index was reset to keep the dataset clean and continuous.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e93ea8",
   "metadata": {},
   "source": [
    "### Checking for duplicated rows\n",
    "- We check for duplicate rows in the dataset. Duplicate records could distort model learning and skew metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e22e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicates}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc56f40",
   "metadata": {},
   "source": [
    "## Checking for missing values\n",
    "- `.info()` earlier said everything was non-null. We know that the \"TotalCharges\" column had empty strings which pandas doesn't count as missing.\n",
    "-We go deeper looking for empty strings or white spaces pretending to be real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be18c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values (NaNs)\n",
    "print(\"Missing values (NaNs):\\n\", df.isna().sum())\n",
    "\n",
    "# Check for empty strings or spaces in object columns\n",
    "print(\"\\nEmpty strings or whitespace in object columns:\\n\")\n",
    "for col in df.select_dtypes(include='object'):\n",
    "    empty_count = df[col].apply(lambda x: isinstance(x, str) and x.strip() == \"\").sum()\n",
    "    if empty_count > 0:\n",
    "        print(f\"{col}: {empty_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3945b61",
   "metadata": {},
   "source": [
    "-We verified the dataset for both traditional missing values (NaN) and hidden missing indicators like empty strings or whitespaces.\n",
    "-No NaN values were found in any column.\n",
    "-No object-type columns contained empty or whitespace-only entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49e12dd",
   "metadata": {},
   "source": [
    "### Strip Whitespace from Object Columns\n",
    "In this step, we remove any leading or trailing spaces from all string-based (object) columns to ensure consistency (e.g., no accidental \"Yes \" vs. \"Yes\"). After stripping, weâ€™ll re-print unique values for each object column to confirm that no stray spaces remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f24e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip leading/trailing whitespace from all object-type columns\n",
    "for col in df.select_dtypes(include=\"object\"):\n",
    "    df[col] = df[col].str.strip()\n",
    "\n",
    "# Re-print unique values for each object column to verify\n",
    "for col in df.select_dtypes(include=\"object\"):\n",
    "    print(f\"\\nUnique values in '{col}':\")\n",
    "    print(sorted(df[col].unique()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5d7543",
   "metadata": {},
   "source": [
    "## Whitespace Stripping & Category Review\n",
    "\n",
    "- After stripping leading/trailing spaces, we re-inspected each object-type columnâ€™s unique values.\n",
    "- **`customerID`**: All values look uniform (alphanumeric IDs), so no residual whitespace issues here.\n",
    "- **Binary â€œYes/Noâ€ columns** (e.g., `gender`, `Partner`, `Dependents`, `PhoneService`, `PaperlessBilling`, `Churn`) now correctly show only `'Yes'` or `'No'`.\n",
    "- For **service-related columns** (`MultipleLines`, `OnlineSecurity`, `OnlineBackup`, `DeviceProtection`, `TechSupport`, `StreamingTV`, `StreamingMovies`), we still see entries like `\"No phone service\"` or `\"No internet service\"`. Those are legitimate labels (indicating the customer doesnâ€™t have that service), but we need to decide how to encode them:\n",
    "  - Since these columns are effectively binary (has service vs. doesnâ€™t), weâ€™ll consolidate `\"No phone service\"` ->`\"No\"` and `\"No internet service\"` -> `\"No\"` in the next step.\n",
    "- **`InternetService`** values are `['DSL', 'Fiber optic', 'No']`, which aligns with expectations (no extra whitespace).\n",
    "- **`Contract`** values are `['Month-to-month', 'One year', 'Two year']`(no stray spaces).\n",
    "- **`PaymentMethod`** shows all four expected categories (`'Bank transfer (automatic)'`, `'Credit card (automatic)'`, `'Electronic check'`, `'Mailed check'`), so no further trimming is needed.\n",
    "\n",
    "**Conclusion:** Whitespace cleanup succeeded. Our next move is to standardize service-related columns by mapping `\"No phone service\"` and `\"No internet service\"` to a simple `\"No\"`, preparing them for binary encoding.```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673b85c7",
   "metadata": {},
   "source": [
    "##Consolidate Service Labels (\"No phone service\" / \"No internet service\" -> \"No\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d8af1b",
   "metadata": {},
   "source": [
    "-This is done because all 7 service-related columns are binary meaning: Either the customer has the service or not.\n",
    "-We aim to standardize all 7 service-related columns to only have \"Yes\" or \"No\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc66f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidate \"No phone service\" and \"No internet service\" to \"No\"\n",
    "replace_cols = ['MultipleLines', 'OnlineSecurity', 'OnlineBackup', \n",
    "                'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies']\n",
    "\n",
    "# Replace both \"No phone service\" and \"No internet service\" with \"No\"\n",
    "for col in replace_cols:\n",
    "    df[col] = df[col].replace({'No phone service': 'No', 'No internet service': 'No'})\n",
    "\n",
    "# Double-check the unique values now\n",
    "for col in replace_cols:\n",
    "    print(f\"\\nUnique values in '{col}': {sorted(df[col].unique())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d0b428",
   "metadata": {},
   "source": [
    "### Service Label Consolidation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55b185c",
   "metadata": {},
   "source": [
    "\n",
    "- We simplified category values across 7 service-related columns:\n",
    "  - `MultipleLines`\n",
    "  - `OnlineSecurity`\n",
    "  - `OnlineBackup`\n",
    "  - `DeviceProtection`\n",
    "  - `TechSupport`\n",
    "  - `StreamingTV`\n",
    "  - `StreamingMovies`\n",
    "\n",
    "- After the replacements, all 7 columns now contain **only `'Yes'` or `'No'`**, making them ready for encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfccd674",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirming if any missing values exist or unexpected issues arose afterwards \n",
    "print(df.info())\n",
    "print(df.isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c3ba5e",
   "metadata": {},
   "source": [
    "### After all cleaning steps, no columns contain nulls and all dtypes are correct. The dataset is ready for feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314f7613",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the cleaned file to a csv \n",
    "df.to_csv(\"../data/customer_churn_cleaned.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
